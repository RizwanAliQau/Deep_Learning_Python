{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes XGBoost so popular:\n",
    "\n",
    "1) Speed and Performance : Originally written in C++, it is comparatively faster than other ensemble classifiers.\n",
    "\n",
    "2) Core algorithm is parallelizable : Because the core XGBoost algorithm is parallelizable it can harness the power of multi-core computers. It is also parallelizable onto GPUâ€™s and across networks of computers making it feasible to train on very large datasets as well.\n",
    "\n",
    "3) Consistently outperforms other algorithm methods : It has shown better performance on a variety of machine learning benchmark datasets.\n",
    "\n",
    "4) Wide variety of tuning parameters : XGBoost internally has parameters for cross-validation, regularization, user-defined objective functions, missing values, tree parameters, scikit-learn compatible API etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost (Extreme Gradient Boosting) belongs to a family of boosting algorithms and uses the gradient boosting (GBM) framework at its core. It is an optimized distributed gradient boosting library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read https://towardsdatascience.com/decision-tree-ensembles-bagging-and-boosting-266a8ba60fd9 for decision tree ensemble. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Ensembles (Bagging and Boosting) \n",
    "\n",
    "* Random Forest is parallel algorithm for ensemble (Bagging)\n",
    "* Gradient booting is sequential (Boosting) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial solve a regression problem using XGBoost:\n",
    "\n",
    "The dataset is taken from the UCI Machine Learning Repository and is also present in sklearn's datasets module. It has 14 explanatory variables describing various aspects of residential homes in Boston, the challenge is to predict the median value of owner-occupied homes per $1000s.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : Import Dataset \n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "# the boston is a dictionary so we can check its keys \n",
    "print(boston.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data into dataframes \n",
    "import pandas as pd\n",
    "data = pd.DataFrame(boston.data)\n",
    "data.columns = boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append boston.target to your pandas DataFrame.\n",
    "data['PRICE'] = boston.target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  PRICE  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      "CRIM       506 non-null float64\n",
      "ZN         506 non-null float64\n",
      "INDUS      506 non-null float64\n",
      "CHAS       506 non-null float64\n",
      "NOX        506 non-null float64\n",
      "RM         506 non-null float64\n",
      "AGE        506 non-null float64\n",
      "DIS        506 non-null float64\n",
      "RAD        506 non-null float64\n",
      "TAX        506 non-null float64\n",
      "PTRATIO    506 non-null float64\n",
      "B          506 non-null float64\n",
      "LSTAT      506 non-null float64\n",
      "PRICE      506 non-null float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "# to get the useful information about the data \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677083</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT       PRICE  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that describe() only gives summary statistics of columns which are \n",
    "# continuous in nature and not categorical.\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will build the model using Trees as base learners (which are the default base learners) using XGBoost's scikit-learn compatible API. Along the way, you will also learn some of the common tuning parameters which XGBoost provides in order to improve the model's performance, and using the root mean squared error (RMSE) performance metric to check the performance of the trained model on the test set. Root mean Squared error is the square root of the mean of the squared differences between the actual and the predicted values. As usual, you start by importing the library xgboost and other important libraries that you will be using for building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the target variable and rest of the variables using .iloc \n",
    "# to subset the data.\n",
    "X,y = data.iloc[:,:-1],data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. You will use this later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Rizwan\\anaconda3\\envs\\machine learning\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Rizwan\\anaconda3\\envs\\machine learning\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    }
   ],
   "source": [
    "data_dmatrix = xgb.DMatrix(data= X, label=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost's hyperparameters:\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them here. But the most common ones that you should know are:\n",
    "* learning_rate: step size shrinkage used to prevent overfitting. Range is [0,1]\n",
    "* max_depth: determines how deeply each tree is allowed to grow during any boosting round.\n",
    "* subsample: percentage of samples used per tree. Low value can lead to underfitting.\n",
    "* colsample_bytree: percentage of features used per tree. High value can lead to overfitting.\n",
    "* n_estimators: number of trees you want to build.\n",
    "* objective: determines the loss function to be used like reg:linear for regression problems, reg:logistic for classification problems with only decision, binary:logistic for classification problems with probability.\n",
    "\n",
    "XGBoost also supports regularization parameters to penalize models as they become more complex and reduce them to simple (parsimonious) models:\n",
    "* gamma: controls whether a given node will split based on the expected reduction in loss after the split. A higher value leads to fewer splits. Supported only for tree-based learners.\n",
    "* alpha: L1 regularization on leaf weights. A large value leads to more regularization.\n",
    "* lambda: L2 regularization on leaf weights and is smoother than L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test split from sklearn library \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGboost initialisation (For classification problems, you would\n",
    "# have used the XGBClassifier() class.)\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear',colsample_bytree=0.3,\n",
    "                          learning_rate=0.1,max_depth=5,alpha=10,\n",
    "                          n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Rizwan\\anaconda3\\envs\\machine learning\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:05:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=5, min_child_weight=1, missing=None, n_estimators=10,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the regressor to the training set and make predictions on the test\n",
    "# set using the familiar .fit() and .predict() methods.\n",
    "xg_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xg_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 10.449300\n"
     ]
    }
   ],
   "source": [
    "# Compute the rmse by invoking the mean_sqaured_error function from sklearn's\n",
    "# metrics module.\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"RMSE: %f\" %(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: Well, you can see that your RMSE for the price prediction came out to be around 10.8 per 1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "# In order to build more robust models, it is common to do a k-fold cross \n",
    "# validation where all the entries in the original training dataset are\n",
    "# used for both training as well as validation.  Also, each entry is used\n",
    "# for validation just once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters, few pf them are:\n",
    "\n",
    "* num_boost_round: denotes the number of trees you build (analogous to n_estimators)\n",
    "* metrics: tells the evaluation metrics to be watched during CV\n",
    "* as_pandas: to return the results in a pandas DataFrame.\n",
    "* early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "* seed: for reproducibility of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:31:48] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:31:48] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:31:48] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "# You will use these parameters to build a 3-fold cross validation model\n",
    "# by invoking XGBoost's cv() method and store the results in a cv_results \n",
    "# DataFrame. Note that here you are using the Dmatrix object you created \n",
    "# before.\n",
    "\n",
    "params = {\"objective\": \"reg:linear\",'colsample_bytree':0.3,'learning_rae':0.1,\n",
    "          'max_depth':5,'alpha':10}\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, params= params,nfold = 3,\n",
    "                    num_boost_round= 50,early_stopping_rounds=10,\n",
    "                    metrics='rmse', as_pandas= True, seed=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-rmse-mean</th>\n",
       "      <th>train-rmse-std</th>\n",
       "      <th>test-rmse-mean</th>\n",
       "      <th>test-rmse-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.352567</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>17.354313</td>\n",
       "      <td>0.001677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.998922</td>\n",
       "      <td>0.056980</td>\n",
       "      <td>12.994745</td>\n",
       "      <td>0.107389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.971335</td>\n",
       "      <td>0.219584</td>\n",
       "      <td>10.044722</td>\n",
       "      <td>0.323191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.674173</td>\n",
       "      <td>0.046481</td>\n",
       "      <td>7.967775</td>\n",
       "      <td>0.201156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.238753</td>\n",
       "      <td>0.145578</td>\n",
       "      <td>6.720836</td>\n",
       "      <td>0.340294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.194406</td>\n",
       "      <td>0.153373</td>\n",
       "      <td>5.912716</td>\n",
       "      <td>0.297667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.416165</td>\n",
       "      <td>0.045499</td>\n",
       "      <td>5.427787</td>\n",
       "      <td>0.227750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.887400</td>\n",
       "      <td>0.084919</td>\n",
       "      <td>5.141721</td>\n",
       "      <td>0.242972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.576676</td>\n",
       "      <td>0.084330</td>\n",
       "      <td>4.935209</td>\n",
       "      <td>0.232458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.253552</td>\n",
       "      <td>0.096864</td>\n",
       "      <td>4.668678</td>\n",
       "      <td>0.301483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.007156</td>\n",
       "      <td>0.132495</td>\n",
       "      <td>4.504849</td>\n",
       "      <td>0.311346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.796084</td>\n",
       "      <td>0.106540</td>\n",
       "      <td>4.334309</td>\n",
       "      <td>0.369668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.639365</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>4.264040</td>\n",
       "      <td>0.427003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.536010</td>\n",
       "      <td>0.110936</td>\n",
       "      <td>4.191467</td>\n",
       "      <td>0.453793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.412033</td>\n",
       "      <td>0.115814</td>\n",
       "      <td>4.120674</td>\n",
       "      <td>0.460393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.341696</td>\n",
       "      <td>0.134183</td>\n",
       "      <td>4.079198</td>\n",
       "      <td>0.446943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.273317</td>\n",
       "      <td>0.148552</td>\n",
       "      <td>4.059398</td>\n",
       "      <td>0.448778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.221226</td>\n",
       "      <td>0.142710</td>\n",
       "      <td>4.051149</td>\n",
       "      <td>0.436308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.166224</td>\n",
       "      <td>0.136667</td>\n",
       "      <td>4.039532</td>\n",
       "      <td>0.434820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.107298</td>\n",
       "      <td>0.102647</td>\n",
       "      <td>4.006031</td>\n",
       "      <td>0.479889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.074059</td>\n",
       "      <td>0.086904</td>\n",
       "      <td>3.999820</td>\n",
       "      <td>0.477287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.036581</td>\n",
       "      <td>0.093350</td>\n",
       "      <td>3.987601</td>\n",
       "      <td>0.479272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.990072</td>\n",
       "      <td>0.099145</td>\n",
       "      <td>3.959077</td>\n",
       "      <td>0.479056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.958469</td>\n",
       "      <td>0.094050</td>\n",
       "      <td>3.942003</td>\n",
       "      <td>0.495260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.926283</td>\n",
       "      <td>0.099811</td>\n",
       "      <td>3.922544</td>\n",
       "      <td>0.493882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.890912</td>\n",
       "      <td>0.123324</td>\n",
       "      <td>3.906204</td>\n",
       "      <td>0.494643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.867575</td>\n",
       "      <td>0.114263</td>\n",
       "      <td>3.892622</td>\n",
       "      <td>0.503652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.824950</td>\n",
       "      <td>0.110028</td>\n",
       "      <td>3.886433</td>\n",
       "      <td>0.497140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.791396</td>\n",
       "      <td>0.102162</td>\n",
       "      <td>3.877158</td>\n",
       "      <td>0.496932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.763696</td>\n",
       "      <td>0.089245</td>\n",
       "      <td>3.871707</td>\n",
       "      <td>0.505859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.731941</td>\n",
       "      <td>0.093261</td>\n",
       "      <td>3.866701</td>\n",
       "      <td>0.506961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.709871</td>\n",
       "      <td>0.095667</td>\n",
       "      <td>3.853240</td>\n",
       "      <td>0.508006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.682251</td>\n",
       "      <td>0.095075</td>\n",
       "      <td>3.838184</td>\n",
       "      <td>0.518005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.660485</td>\n",
       "      <td>0.097301</td>\n",
       "      <td>3.824640</td>\n",
       "      <td>0.514140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.635309</td>\n",
       "      <td>0.106810</td>\n",
       "      <td>3.816001</td>\n",
       "      <td>0.518732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.619657</td>\n",
       "      <td>0.102209</td>\n",
       "      <td>3.806889</td>\n",
       "      <td>0.519350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.595486</td>\n",
       "      <td>0.100415</td>\n",
       "      <td>3.796659</td>\n",
       "      <td>0.517983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.572061</td>\n",
       "      <td>0.100825</td>\n",
       "      <td>3.795983</td>\n",
       "      <td>0.516376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.555290</td>\n",
       "      <td>0.103423</td>\n",
       "      <td>3.796415</td>\n",
       "      <td>0.520574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.529004</td>\n",
       "      <td>0.092140</td>\n",
       "      <td>3.785631</td>\n",
       "      <td>0.521813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.512385</td>\n",
       "      <td>0.087286</td>\n",
       "      <td>3.776641</td>\n",
       "      <td>0.519777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.500338</td>\n",
       "      <td>0.078184</td>\n",
       "      <td>3.771188</td>\n",
       "      <td>0.517043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.486420</td>\n",
       "      <td>0.069340</td>\n",
       "      <td>3.767111</td>\n",
       "      <td>0.511965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.479559</td>\n",
       "      <td>0.067317</td>\n",
       "      <td>3.763913</td>\n",
       "      <td>0.509346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.469456</td>\n",
       "      <td>0.075571</td>\n",
       "      <td>3.762031</td>\n",
       "      <td>0.510307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.462504</td>\n",
       "      <td>0.071889</td>\n",
       "      <td>3.761592</td>\n",
       "      <td>0.511329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.449693</td>\n",
       "      <td>0.071466</td>\n",
       "      <td>3.760019</td>\n",
       "      <td>0.513330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.442227</td>\n",
       "      <td>0.076878</td>\n",
       "      <td>3.759967</td>\n",
       "      <td>0.516662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.435911</td>\n",
       "      <td>0.080551</td>\n",
       "      <td>3.759967</td>\n",
       "      <td>0.514564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.426736</td>\n",
       "      <td>0.074445</td>\n",
       "      <td>3.756650</td>\n",
       "      <td>0.513955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
       "0         17.352567        0.012048       17.354313       0.001677\n",
       "1         12.998922        0.056980       12.994745       0.107389\n",
       "2          9.971335        0.219584       10.044722       0.323191\n",
       "3          7.674173        0.046481        7.967775       0.201156\n",
       "4          6.238753        0.145578        6.720836       0.340294\n",
       "5          5.194406        0.153373        5.912716       0.297667\n",
       "6          4.416165        0.045499        5.427787       0.227750\n",
       "7          3.887400        0.084919        5.141721       0.242972\n",
       "8          3.576676        0.084330        4.935209       0.232458\n",
       "9          3.253552        0.096864        4.668678       0.301483\n",
       "10         3.007156        0.132495        4.504849       0.311346\n",
       "11         2.796084        0.106540        4.334309       0.369668\n",
       "12         2.639365        0.131148        4.264040       0.427003\n",
       "13         2.536010        0.110936        4.191467       0.453793\n",
       "14         2.412033        0.115814        4.120674       0.460393\n",
       "15         2.341696        0.134183        4.079198       0.446943\n",
       "16         2.273317        0.148552        4.059398       0.448778\n",
       "17         2.221226        0.142710        4.051149       0.436308\n",
       "18         2.166224        0.136667        4.039532       0.434820\n",
       "19         2.107298        0.102647        4.006031       0.479889\n",
       "20         2.074059        0.086904        3.999820       0.477287\n",
       "21         2.036581        0.093350        3.987601       0.479272\n",
       "22         1.990072        0.099145        3.959077       0.479056\n",
       "23         1.958469        0.094050        3.942003       0.495260\n",
       "24         1.926283        0.099811        3.922544       0.493882\n",
       "25         1.890912        0.123324        3.906204       0.494643\n",
       "26         1.867575        0.114263        3.892622       0.503652\n",
       "27         1.824950        0.110028        3.886433       0.497140\n",
       "28         1.791396        0.102162        3.877158       0.496932\n",
       "29         1.763696        0.089245        3.871707       0.505859\n",
       "30         1.731941        0.093261        3.866701       0.506961\n",
       "31         1.709871        0.095667        3.853240       0.508006\n",
       "32         1.682251        0.095075        3.838184       0.518005\n",
       "33         1.660485        0.097301        3.824640       0.514140\n",
       "34         1.635309        0.106810        3.816001       0.518732\n",
       "35         1.619657        0.102209        3.806889       0.519350\n",
       "36         1.595486        0.100415        3.796659       0.517983\n",
       "37         1.572061        0.100825        3.795983       0.516376\n",
       "38         1.555290        0.103423        3.796415       0.520574\n",
       "39         1.529004        0.092140        3.785631       0.521813\n",
       "40         1.512385        0.087286        3.776641       0.519777\n",
       "41         1.500338        0.078184        3.771188       0.517043\n",
       "42         1.486420        0.069340        3.767111       0.511965\n",
       "43         1.479559        0.067317        3.763913       0.509346\n",
       "44         1.469456        0.075571        3.762031       0.510307\n",
       "45         1.462504        0.071889        3.761592       0.511329\n",
       "46         1.449693        0.071466        3.760019       0.513330\n",
       "47         1.442227        0.076878        3.759967       0.516662\n",
       "48         1.435911        0.080551        3.759967       0.514564\n",
       "49         1.426736        0.074445        3.756650       0.513955"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49    3.75665\n",
      "Name: test-rmse-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Extract and print the final boosting round metric.\n",
    "\n",
    "print((cv_results[\"test-rmse-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: You can see that your RMSE for the price prediction has reduced \n",
    "as compared to last time and came out to be around 4.03 per 1000$. \n",
    "You can reach an even lower RMSE for a different set of hyper-parameters. \n",
    "You may consider applying techniques like Grid Search, Random Search and \n",
    "Bayesian Optimization to reach the optimal set of hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Boosting Trees and Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also visualize individual trees from the fully boosted model that XGBoost creates using the entire housing dataset. XGBoost has a plot_tree() function that makes this type of visualization easy. Once you train a model using the XGBoost learning API, you can pass it to the plot_tree() function along with the number of trees you want to plot using the num_trees argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:59:48] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting the first tree using matplotlib library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import matplotlib.pyplot as plt\\n\\nxgb.plot_tree(xg_reg,num_trees=0)\\nplt.rcParams['figure.figsize'] = [50, 10]\\nplt.show()\""
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "\n",
    "xgb.plot_tree(xg_reg,num_trees=0)\n",
    "plt.rcParams['figure.figsize'] = [50, 10]\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to visualize your XGBoost models is to examine the \n",
    "# importance of each feature column in the original dataset within the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple way of doing this involves counting the number of times each feature is split on across all boosting rounds (trees) in the model, and then visualizing the result as a bar graph, with the features ordered according to how many times they appear. XGBoost has a plot_importance() function that allows you to do exactly this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_reg)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: As you can see the feature RM has been given the highest importance score among all the features. Thus XGBoost also gives you a way to do Feature Selection. Isn't this brilliant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<------------------------------------------- :) :) --------------------->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is preferable:\n",
    "* when we have structured data \n",
    "* when we have a large amount of data (more than 100 examples)\n",
    "* the number of features are less than number of training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
